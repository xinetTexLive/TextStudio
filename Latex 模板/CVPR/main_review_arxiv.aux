\relax 
\citation{yu2016modeling}
\citation{mao2016generation}
\citation{phloc}
\citation{wang2016structured}
\citation{rohrbach2016grounding}
\citation{kazemzadeh2014referitgame}
\citation{mao2016generation}
\citation{yu2016modeling}
\citation{mao2016generation}
\citation{hu2016natural}
\citation{Nagaraja2016}
\citation{phloc}
\citation{wang2016structured}
\citation{Logic:conversation}
\providecommand \oddpage@label [2]{}
\providecommand\tcolorbox@label[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}}
\newlabel{sec:intro}{{1}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A human issuing a referring expression while gazing at the object in the scene. The system combines multiple modalities such as appearance, motion and stereo depth from the video, along with the expression and the gaze to localize the object.\relax }}{1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:teaser}{{1}{1}}
\citation{deep:semantic:alignment:cvpr15}
\citation{show:attend:tell}
\citation{caption:back}
\citation{exploraing:models:data:vqa}
\citation{VQA}
\citation{andreas2016learning}
\citation{frome2013devise}
\citation{deep:semantic:alignment:cvpr15}
\citation{multimodal:pooling}
\citation{densecap:cvpr16}
\citation{rohrbach2016grounding}
\citation{yu2016modeling}
\citation{mao2016generation}
\citation{frome2013devise}
\citation{long-term:recurrent}
\citation{deep:semantic:alignment:cvpr15}
\citation{exploraing:models:data:vqa}
\citation{ask:your:neurons}
\citation{multimodal:pooling}
\citation{glove}
\citation{hu2016natural}
\citation{mao2016generation}
\citation{VQA}
\citation{exploraing:models:data:vqa}
\citation{yu2016modeling}
\citation{mao2016generation}
\citation{phloc}
\citation{wang2016structured}
\citation{rohrbach2016grounding}
\citation{hu2016natural}
\citation{hu2016segmentation}
\citation{multimodal:pooling}
\citation{deep:semantic:alignment:cvpr15}
\citation{mao2016generation}
\citation{hu2016natural}
\citation{Nagaraja2016}
\citation{yu2016modeling}
\citation{mao2016generation}
\citation{hu2016natural}
\citation{mao2016generation}
\citation{yu2016modeling}
\citation{kazemzadeh2014referitgame}
\citation{mao2016generation}
\citation{lin2014microsoft}
\citation{kazemzadeh2014referitgame}
\citation{karthikeyan2013and}
\citation{yun2013studying}
\citation{Vadivel_2015_CVPR}
\citation{sugano2016seeing}
\citation{Vadivel_2015_CVPR}
\citation{papadopoulos2014training}
\citation{soliman2016towards}
\citation{misu2013situated}
\citation{Vadivel_2015_CVPR}
\citation{papadopoulos2014training}
\citation{nips15_recasens}
\citation{krafka2016eye}
\citation{krafka2016eye}
\citation{hu2016natural}
\citation{mao2016generation}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Related Work}{2}}
\newlabel{sec:related}{{2}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}Approach}{2}}
\newlabel{sec:approach}{{3}{2}}
\citation{hu2016natural}
\citation{densecap:cvpr16}
\citation{hu2016natural}
\citation{hu2016natural}
\citation{lstm}
\citation{hu2016natural}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The illustrative diagram of our model for object referring in stereo videos with language expression and human gaze. Given a referring expression $S$, our model scores all the $M$ bounding box candidates by jointly considering local appearance ($I_{\text  {box}}^t$), local motion ($O_{\text  {box}}^t$), local depth ($D_{\text  {box}}^t$), local human gaze ($G_{\text  {box}}^t$), spatial configuration $X_{\text  {spatial}}$, and the global temporal-spatial contextual information ($I^t$, $D^t$ and $O^t$).\relax }}{3}}
\newlabel{fig:pipeline}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\hskip -1em.\nobreakspace  {}Network Architecture}{3}}
\newlabel{sec:model}{{3.1}{3}}
\citation{hu2016natural}
\citation{ZitnickECCV14edgeBoxes}
\citation{mao2016generation}
\citation{deep:semantic:alignment:cvpr15}
\citation{renNIPS15fasterrcnn}
\citation{he2017mask}
\citation{vasudevan2017chi}
\citation{vasudevan2017chi}
\citation{vasudevan2017chi}
\citation{vgg16}
\citation{imagenet:2015}
\citation{rgbd:net:eccv14}
\citation{rgbd:net:eccv14}
\citation{two:stream:nips14}
\citation{glove}
\citation{lstm}
\citation{kowalski2017deep}
\citation{krafka2016eye}
\citation{krafka2016eye}
\citation{hu2016natural}
\citation{mao2016generation}
\citation{hu2016natural}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}\hskip -1em.\nobreakspace  {}Object Proposals}{4}}
\newlabel{sec:objprop}{{3.2}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}\hskip -1em.\nobreakspace  {}Feature Encoding}{4}}
\newlabel{sec:featureencod}{{3.3}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}Dataset Annotation}{4}}
\newlabel{sec:dataset}{{4}{4}}
\citation{cityscape}
\citation{li2016tgif}
\citation{krafka2016eye}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Top: sample images from the Cityscape dataset with objects marked in differently colored bounding boxes. Bottom: corresponding referring expression annotations.\relax }}{5}}
\newlabel{fig:recallvsiou}{{3}{5}}
\citation{vasudevan2017chi}
\citation{vgg16}
\citation{rgbd:net:eccv14}
\citation{rgbd:net:eccv14}
\citation{kroeger2016fast}
\citation{two:stream:nips14}
\citation{valmadre2017end}
\citation{viola2001rapid}
\citation{kowalski2017deep}
\citation{krafka2016eye}
\citation{krafka2016eye}
\citation{phloc}
\citation{hu2016natural}
\citation{cityscape}
\citation{hu2016natural}
\citation{mao2016generation}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.\nobreakspace  {}Experiments}{6}}
\newlabel{sec:experiment}{{5}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}\hskip -1em.\nobreakspace  {}Implementation Details}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Numbers denote Acc@1. The \# of candidate proposals $M$ is $30$. All evaluations are on Cityscapes. Abbreviations: I:RGB, D:Depth map, O:Optical Flow, G:Gaze. Since Edgebox performs poorly in baseline:Ours(I), we avoid further experiments.\relax }}{6}}
\newlabel{tab:eop-expt}{{1}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Comparison of methods when longer term motion is considered. Numbers denote Acc@1. Track length represents the number of past frames used for flow information. The different methods are evaluated on Cityscape.\relax }}{6}}
\newlabel{tab:cityscape-track}{{2}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}\hskip -1em.\nobreakspace  {}Evaluation}{6}}
\citation{ZitnickECCV14edgeBoxes}
\citation{renNIPS15fasterrcnn}
\citation{vasudevan2017chi}
\citation{hu2016natural}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Comparison of approaches w/ and w/o Gaze. Numbers denote Acc@1. \# of candidate proposals $M$ is 30. The different methods are evaluated on Cityscape. 1st row has overlap with Tab.\nobreakspace  {}1\hbox {}\relax }}{7}}
\newlabel{tab:gaze-comparison}{{3}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Gaze Estimation error distribution. We compute the distance between the gaze estimation coordinates with the groundtruth bounding box along X and Y axis(Centre denotes zero error). Left side figure represents the error in real valued scale and right side in log scale. We choose 2000 pixel distance to match with Cityscapes image dimensions.\relax }}{7}}
\newlabel{fig:gaze_error}{{4}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Some qualitative results from: NLOR (left column), Ours(I,D,O) (middle) and Ours(I,D,O,G) (right column). These results are obtained on the Cityscapes. {\color  {green} Green}: ground truth box and {\color  {red}Red}: predicted box.\relax }}{7}}
\newlabel{fig:qual-results-cityscape}{{5}{7}}
\bibstyle{ieee}
\bibdata{egbib}
\bibcite{andreas2016learning}{1}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Overall results on Cityscapes. Input descriptions on the first row and input videoframe and gaze in the second row. Middle row represents intermediate results where Gaze estimation is embedded along with object proposals while bottom row represents the final OR results. {\color  {green} Green}: ground truth box and {\color  {red}Red}: proposals and the predicted boxes.\relax }}{8}}
\newlabel{fig:qual-gaze-results-cityscape}{{6}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {6}\hskip -1em.\nobreakspace  {}Conclusions}{8}}
\newlabel{sec:conclusion}{{6}{8}}
\bibcite{VQA}{2}
\bibcite{cityscape}{3}
\bibcite{long-term:recurrent}{4}
\bibcite{caption:back}{5}
\bibcite{frome2013devise}{6}
\bibcite{multimodal:pooling}{7}
\bibcite{Logic:conversation}{8}
\bibcite{rgbd:net:eccv14}{9}
\bibcite{he2017mask}{10}
\bibcite{lstm}{11}
\bibcite{hu2016segmentation}{12}
\bibcite{hu2016natural}{13}
\bibcite{densecap:cvpr16}{14}
\bibcite{deep:semantic:alignment:cvpr15}{15}
\bibcite{karthikeyan2013and}{16}
\bibcite{kazemzadeh2014referitgame}{17}
\bibcite{kowalski2017deep}{18}
\bibcite{krafka2016eye}{19}
\bibcite{kroeger2016fast}{20}
\bibcite{li2016tgif}{21}
\bibcite{lin2014microsoft}{22}
\bibcite{ask:your:neurons}{23}
\bibcite{mao2016generation}{24}
\bibcite{misu2013situated}{25}
\bibcite{Nagaraja2016}{26}
\bibcite{papadopoulos2014training}{27}
\bibcite{glove}{28}
\bibcite{phloc}{29}
\bibcite{nips15_recasens}{30}
\bibcite{exploraing:models:data:vqa}{31}
\bibcite{renNIPS15fasterrcnn}{32}
\bibcite{rohrbach2016grounding}{33}
\bibcite{imagenet:2015}{34}
\bibcite{Vadivel_2015_CVPR}{35}
\bibcite{two:stream:nips14}{36}
\bibcite{vgg16}{37}
\bibcite{soliman2016towards}{38}
\bibcite{sugano2016seeing}{39}
\bibcite{valmadre2017end}{40}
\bibcite{vasudevan2017chi}{41}
\bibcite{viola2001rapid}{42}
\bibcite{wang2016structured}{43}
\bibcite{show:attend:tell}{44}
\bibcite{yu2016modeling}{45}
\bibcite{yun2013studying}{46}
\bibcite{ZitnickECCV14edgeBoxes}{47}
